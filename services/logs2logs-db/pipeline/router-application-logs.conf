input {
  udp {
    port => 5140
    queue_size => 5000
    receive_buffer_bytes => 26214400
  }
  lumberjack {
    port => 5044
    ssl_certificate => "certs/lumberjack.cert"
    ssl_key => "certs/lumberjack.key"
    codec => json
  }
}

filter {
  if [message] =~ /^{.*}/ {
    # message is JSON, this is an application log
    mutate {
      add_field => { "[@metadata][log-type]" => "application-logs" }
    }
    json {
      source => "message"
    }
    # type contains the name of the openshift project
    if ![type] {
      mutate {
        add_field => { "openshift_project" => "noproject" }
      }
    } else {
      mutate {
        add_field => { "openshift_project" => "%{type}" }
        remove_field => [ "type" ]
      }
    }
  } else {
    # Not JSON, therefore a syslog and a router log entry
    mutate {
      add_field => { "[@metadata][log-type]" => "router-logs" }
    }
    grok {
      match => ["message", "(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"]
    }
    # If the above grok has an error and it contains "be_sni/" or "be_no_sni/" inside the message, this request is an internal haproxy forward for HTTPs requests
    # we can savely drop them as they don't contain any information and will be logged twice anyway
    if "_grokparsefailure" in [tags] and [message] =~ /(be_sni\/)|(be_no_sni\/)/ {
      drop { }
    }
    grok {
      match => ["captured_request_headers", "%{URIHOST:request_header_host}\|%{GREEDYDATA:request_header_useragent}"]
    }
    grok {
      match => {"backend_name" => [ "%{NOTSPACE:haproxy_backend}:%{NOTSPACE:openshift_project}:%{NOTSPACE:openshift_route}", "%{NOTSPACE:openshift_project}" ] }
    }
    grok {
      match => {"server_name" => [ "pod:%{NOTSPACE:openshift_pod}:%{NOTSPACE:openshift_service}:%{NOTSPACE:openshift_pod_ip}:%{NOTSPACE:openshift_pod_port}", "%{NOTSPACE:server_name}" ] }
    }
    if ![openshift_project] {
      mutate {
        add_field => { "openshift_project" => "errors" }
      }
    } else {
      mutate {
        remove_field => [ "message" ]
      }
    }
    # We run automated testing which creates project names in the form of e2e-0000000000-AAAAAAAAAAAAAAAAAAAAAAAAAA
    # we just save them as with the project name 'e2'
    if [openshift_project] =~ /^e2e-\d{10}-\S{26}$/ {
      mutate {
        update => { "openshift_project" => "e2e" }
      }
    }
  }

  mutate {
    # within elasticsearch all projects are lowercase
    lowercase => [ "openshift_project" ]
  }
  mutate {
    gsub => [
      # within elasticsearch we don't support any special characters except dashes, convert openshift_project to this
      "openshift_project", "[^0-9a-z-]", "-"
    ]
  }
}

output {
  # stdout { codec => rubydebug }
  if [@metadata][log-type] == "router-logs" {
    elasticsearch {
      user => admin
      password => "${LOGSDB_ADMIN_PASSWORD}"
      hosts => ["${ELASTICSEARCH_URL}"]
      index => "router-logs-%{[openshift_project]}-%{+YYYY.MM}"
      template => "/usr/share/logstash/templates/router-logs.json"
      template_name => "router-logs"
      template_overwrite => true
    }
  } else {
    # stdout { codec => rubydebug }
    elasticsearch {
      user => admin
      password => "${LOGSDB_ADMIN_PASSWORD}"
      hosts => ["${ELASTICSEARCH_URL}"]
      index => "application-logs-%{[openshift_project]}-%{+YYYY.MM}"
      template => "/usr/share/logstash/templates/application-logs.json"
      template_name => "application-logs"
      template_overwrite => true
    }
  }

}
